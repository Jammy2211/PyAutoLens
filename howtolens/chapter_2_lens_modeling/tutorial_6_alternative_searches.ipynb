{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Tutorial 6: Alternative Searches\n",
        "================================\n",
        "\n",
        "Up to now, we've always used the `NonLinearSearch` Dynesty and not considered the input parameters that control its\n",
        "sampling. In this tutorial, we'll consider how we can change these setting to balance finding the global maxima\n",
        "solution with fast run time, as well as other types of non-linear searches we can use to perform lens modeling."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%matplotlib inline\n",
        "\n",
        "from pyprojroot import here\n",
        "\n",
        "workspace_path = str(here())\n",
        "%cd $workspace_path\n",
        "print(f\"Working Directory has been set to `{workspace_path}`\")\n",
        "\n",
        "from os import path\n",
        "import autolens as al\n",
        "import autolens.plot as aplt\n",
        "import autofit as af"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "we'll use new strong lensing data, where:\n",
        "\n",
        " - The lens `Galaxy`'s `LightProfile` is an `EllipticalSersic`.\n",
        " - The lens `Galaxy`'s total mass distribution is an `EllipticalIsothermal`.\n",
        " - The source `Galaxy`'s `LightProfile` is an `EllipticalSersic`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "dataset_name = \"light_sersic__mass_sie__source_sersic\"\n",
        "dataset_path = path.join(\"dataset\", \"howtolens\", \"chapter_2\", dataset_name)\n",
        "\n",
        "imaging = al.Imaging.from_fits(\n",
        "    image_path=path.join(dataset_path, \"image.fits\"),\n",
        "    noise_map_path=path.join(dataset_path, \"noise_map.fits\"),\n",
        "    psf_path=path.join(dataset_path, \"psf.fits\"),\n",
        "    pixel_scales=0.1,\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "we'll create and use a smaller 2.0\" `Mask2D` again."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "mask = al.Mask2D.circular(\n",
        "    shape_2d=imaging.shape_2d, pixel_scales=imaging.pixel_scales, radius=2.6\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "When plotted, the lens light`s is clearly visible in the centre of the image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "imaging_plotter = aplt.ImagingPlotter(\n",
        "    imaging=imaging, visuals_2d=aplt.Visuals2D(mask=mask)\n",
        ")\n",
        "imaging_plotter.subplot_imaging()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Like in the previous tutorial, we use a `SettingsPhaseImaging` object to specify our model-fitting procedure uses a \n",
        "regular `Grid`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "settings_masked_imaging = al.SettingsMaskedImaging(grid_class=al.Grid, sub_size=2)\n",
        "\n",
        "settings = al.SettingsPhaseImaging(settings_masked_imaging=settings_masked_imaging)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Nested Sampling__\n",
        "\n",
        "Lets first perform the model-fit using Dynesty, but look at different parameters that control how long it takes to run. \n",
        "We'll therefore discuss in a bit more detail how Dynesty works, but still keep this description conceptually simple  \n",
        "and avoid technical terms and jargon. For a complete description of Dynesty you should check out the Dynesty \n",
        "publication `https://arxiv.org/abs/1904.02180`.\n",
        "\n",
        "n_live_points:\n",
        "\n",
        "Dynesty is a `nested sampling` algorithm. As we described in tutorial 1, it throws down a set of `live points` in \n",
        "parameter space, where each live point corresponds to a lens model with a given set of parameters. These points are\n",
        "initially distributed according to our priors, hence why tuning our priors allows us to sample parameter space faster.\n",
        " \n",
        "The number of live points is set by the parameter `n_live_points`. More points provide a more thorough sampling of \n",
        "parameter space, increasing the probability that we locate the global maxima solution. Therefore, if you think your \n",
        "model-fit has gone to a local maxima, you should try increasing `n_live_points`. The downside of this is Dynesty will \n",
        "take longer to sample parameter space and converge on a solution. Ideally, we will use as few live points as possible \n",
        "to locate the global maxima as quickly as possible.\n",
        "\n",
        "evidence_tolerance:\n",
        "\n",
        "A nested sampling algorithm estimates the *Bayesian Evidence* of the model-fit, which is quantity the non-linear \n",
        "search algorithms we introduce later do not. The Bayesian evidence quantifies how well the lens model as a whole fits\n",
        "the data, following a principle called Occam's Razor (`https://simple.wikipedia.org/wiki/Occam%27s_razor`). This \n",
        "penalizes models for being more complex (e.g. more parameters) and requires that their additional complexity improve \n",
        "their overall fit to the data compared to a simpler model. By computing the comparing the Bayesian evidence of \n",
        "different models one can objectively choose the lens model that best fits the data.\n",
        "\n",
        "A nested sampling algorithm stops sampling when it estimates that continuing sampling will not increase the Bayesian \n",
        "evidence (called the `log_evidence`) by more than the `evidence_tolerance`. As Dynesty progresses and converges on the\n",
        "solution, the rate of increase of the estimated Bayesian evidence slows down. Therefore, higher `evidence_tolerance`s \n",
        "mean Dynesty terminate sooner.\n",
        "    \n",
        "A high `evidence_tolerance` will make the errors estimated on every parameter unreliable and its value must be kept \n",
        "below 0.8 for reliable error estimates. However, when linking phases, we typically *do not care* about the errors \n",
        "in the first phase, therefore setting a high evidence tolerance can be an effective means to make Dynesty converge\n",
        "faster (we'll estimate reliable errors in the second phase when the `evidence_tolerance is 0.8 or less). \n",
        "\n",
        "walks:\n",
        "\n",
        "By default **PyAutoLens** use's Dynesty's Random Walk nested sampling (other approaches are available in Dynesty \n",
        "however our testing has revealed that the random walk sampling is best for lens modeling). In brief, in order for \n",
        "Dynesty to update the location of a live's point in parameter space, it performs a random walk from the live point's \n",
        "current location for the number of steps defined by the `walks` parameter, with the highest likelihood model chosen. \n",
        "The random walk chooses steps based on the likelihood values it evaluates, so as to try and walk towards higher \n",
        "likelihood solutions.\n",
        "\n",
        "Therefore a `walks` of 5 will walk 5 steps, 10 will take 10 steps and so on. The choice of `walks` needs to strike a \n",
        "balance. If `walks` is too low, the random walk from each live point will not sufficiently explore parameter space \n",
        "around it and therefore fail to locate a new point with a sizeable increase in likelihood. This will slow down \n",
        "convergence around the highest likleihod solutions, but may even mean the global maxima solution is not located at all.\n",
        "If `walks` is too large the walk takes longer than necessary, slowing down the code.\n",
        "\n",
        "Through testing, we have found that `walks` = 5 - 10 is optimal for lens modeling with **PyAutoLens**. Higher values\n",
        "of walks take longer to run, but are better at avoiding Dynesty inferred a local maxima as they more thoroughly\n",
        "sample parameter space. The parameter `facc` controls the size of the steps the random walk takes. We have found \n",
        "**PyAutoLens** performs best for `facc` = 0.2-0.3, but feel free to experiment with these values.\n",
        "\n",
        "\n",
        "Lets perform two fits, where:\n",
        "\n",
        " - One has many live points, a low sampling efficiency and evidence tolerance, causing the `NonLinearSearch` to\n",
        " take a long time to run (in fact, on my laptop, this run takes > 500000 iterations which translates to > 6 \n",
        " hours. So, I've commented the run function out to not waste your time, but feel free to uncomment it and run\n",
        " the phase to see this for yourself!).\n",
        "      \n",
        " - One has few live points, a high sampling efficiency and evidence tolerance, causing the `NonLinearSearch` to\n",
        " converge and end quicker."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "phase_slow = al.PhaseImaging(\n",
        "    search=af.DynestyStatic(\n",
        "        path_prefix=\"howtolens\",\n",
        "        name=\"phase_t6_slow\",\n",
        "        n_live_points=150,\n",
        "        evidence_tolerance=0.8,\n",
        "    ),\n",
        "    settings=settings,\n",
        "    galaxies=af.CollectionPriorModel(\n",
        "        lens=al.GalaxyModel(\n",
        "            redshift=0.5, bulge=al.lp.EllipticalSersic, mass=al.mp.EllipticalIsothermal\n",
        "        ),\n",
        "        source=al.GalaxyModel(redshift=1.0, bulge=al.lp.EllipticalSersic),\n",
        "    ),\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\n",
        "    \"Dynesty has begun running - checkout the workspace/output\"\n",
        "    \"  folder for live output of the results, images and lens model.\"\n",
        "    \"  This Jupyter notebook cell with progress once Dynesty has completed - this could take some time!\"\n",
        ")\n",
        "\n",
        "# This code (and the lines of code after) is commented out to signify the analysis runs slow, so it is optional for\n",
        "# you to run this part of the tutorials.\n",
        "\n",
        "# result_slow = phase_slow.run(dataset=imaging, mask=mask)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Lets check that we get a good model and fit to the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# aplt.FitImaging.subplot_fit_imaging(fit=result_slow.max_log_likelihood_fit)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can use the result to tell us how many iterations Dynesty took to convergence on the solution.\n",
        "\"\"\"\n",
        "print(\"Total Dynesty Iterations (If you skip running the phase, this is ~ 500000):\")\n",
        "# print(result_slow.samples.total_samples)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now lets run the phase with fast setting, so we can compare the total number of iterations required."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "phase_fast = al.PhaseImaging(\n",
        "    search=af.DynestyStatic(\n",
        "        path_prefix=\"howtolens\", name=\"phase_t6_fast\", n_live_points=30\n",
        "    ),\n",
        "    settings=settings,\n",
        "    galaxies=af.CollectionPriorModel(\n",
        "        lens=al.GalaxyModel(\n",
        "            redshift=0.5, bulge=al.lp.EllipticalSersic, mass=al.mp.EllipticalIsothermal\n",
        "        ),\n",
        "        source=al.GalaxyModel(redshift=1.0, bulge=al.lp.EllipticalSersic),\n",
        "    ),\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\n",
        "    \"Dynesty has begun running - checkout the workspace/output\"\n",
        "    \"  folder for live output of the results, images and lens model.\"\n",
        "    \"  This Jupyter notebook cell with progress once Dynesty has completed - this could take some time!\"\n",
        ")\n",
        "\n",
        "# result_fast = phase_fast.run(dataset=imaging, mask=mask)\n",
        "\n",
        "print(\"Dynesty has finished run - you may now continue the notebook.\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Lets check that this search, despite its faster sampling settings, still gives us the global maxima solution."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# aplt.FitImaging.subplot_fit_imaging(fit=result_fast.max_log_likelihood_fit)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And now lets confirm it uses significantly fewer iterations.\n",
        "\"\"\"\n",
        "\n",
        "print(\"Total Dynesty Iterations:\")\n",
        "print(\"Slow settings: ~500000\")\n",
        "# print(result_slow.samples.total_samples)\n",
        "# print(\"Fast settings: \", result_fast.samples.total_samples)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Optimizers__\n",
        "\n",
        "Nested sampling algorithms like Dynesty provides the errors on all of the model parameters, by fully mapping out all \n",
        "of the high likelihood regions of parameter space. This provides knowledge on the complete *range* of models that do \n",
        "and do not provide high likelihood fits to the data, but takes many extra iterations to perform. If we require precise \n",
        "error estimates (perhaps this is our final lens model fit before we publish the results in a paper), these extra\n",
        "iterations are acceptable. \n",
        "\n",
        "However, we often don't care about the errors. For example, in the previous tutorial when linking phases, the only \n",
        "result we used from the fit performed in the first phase was the maximum log likelihood model, omitting the errors\n",
        "entirely! Its seems wasteful to use a nested sampling algorithm like Dynesty to map out the entirity of parameter\n",
        "space when we don't use this information! \n",
        "\n",
        "There are a class of non-linear searches called `optimizers`, which seek to optimize just one thing, the log \n",
        "likelihood. They want to find the model that maximizes the log likelihood, with no regard for the errors, thus not \n",
        "wasting time mapping out in intricate detail every facet of parameter space. Lets see how much faster we can find a \n",
        "good fit to the lens data using an optimizer.\n",
        "\n",
        "we'll use the `Particle Swarm Optimizer` PySwarms. Conceptually this works quite similar to Dynesty, it has a set of \n",
        "points in parameter space (called `particles`) and it uses their likelihoods to determine where it thinks the higher\n",
        "likelihood regions of parameter space are. \n",
        "\n",
        "Unlike Dynesty, this algorithm requires us to specify how many iterations it should perform to find the global \n",
        "maxima solutions. Here, an iteration is the number of samples performed by every particle, so the total number of\n",
        "iterations is n_particles * iters. Lets try a total of ? iterations, a factor ? less than our Dynesty runs above. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "phase_pso = al.PhaseImaging(\n",
        "    search=af.PySwarmsLocal(\n",
        "        path_prefix=\"howtolens\", name=\"phase_t6_pso\", n_particles=50, iters=1000\n",
        "    ),\n",
        "    settings=settings,\n",
        "    galaxies=af.CollectionPriorModel(\n",
        "        lens=al.GalaxyModel(\n",
        "            redshift=0.5, bulge=al.lp.EllipticalSersic, mass=al.mp.EllipticalIsothermal\n",
        "        ),\n",
        "        source=al.GalaxyModel(redshift=1.0, bulge=al.lp.EllipticalSersic),\n",
        "    ),\n",
        ")\n",
        "\n",
        "print(\n",
        "    \"Dynesty has begun running - checkout the workspace/output\"\n",
        "    \"  folder for live output of the results, images and lens model.\"\n",
        "    \"  This Jupyter notebook cell with progress once Dynesty has completed - this could take some time!\"\n",
        ")\n",
        "\n",
        "# result_pso = phase_pso.run(dataset=imaging, mask=mask)\n",
        "\n",
        "print(\"PySwarms has finished run - you may now continue the notebook.\")\n",
        "\n",
        "# aplt.FitImaging.subplot_fit_imaging(fit=result_pso.max_log_likelihood_fit)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It worked, and was much faster than Dynesty!\n",
        "\n",
        "So, when should we use Dynesty and when should we use PySwarms? Its simple:\n",
        "\n",
        " - If we don't care about errors and want to get the global maxima solution as quickly as possible, we should use\n",
        "      PySwarms.\n",
        "      \n",
        " - If we want a model with robust and precise errors, we should use Dynesty.\n",
        "    \n",
        "There is one exception however, for complex models whose priors have not be well tuned or initialized by a previous \n",
        "phase, PySwarms has a tendancy to locate a local maxima. Dynesty`s slower but more complete sampling of parameter space \n",
        "will often find the global maxima when PySwarms doesn`t. So, if you're not happy with the results PySwarms is giving, \n",
        "it may be shrewd to bite-the-button on run-time and use Dynesty to get your initial lens model fit.\n",
        "\n",
        "In the next chapter, when we introduce pipelines, you'll note that are our general strategy to lens modeling is to\n",
        "initialize the model-fit with Dynesty, perform intermediate phases that refine the model with PySwarms and then\n",
        "end with Dynesty for robust errors. Here, we choose our non-linear searches based on what result we want!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__MCMC__\n",
        "\n",
        "For users familiar with Markov Chain Monte Carlo (MCMC) non-linear samplers, PyAutoFit supports the non-linear\n",
        "search *Emcee* (af.Emcee). We have found this to be less effective at lens modeling than Dynesty and PySwarms,\n",
        "but it is sill pretty successful. I've included an example run of Emcee below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "phase_mcmc = al.PhaseImaging(\n",
        "    search=af.Emcee(\n",
        "        path_prefix=\"howtolens\", name=\"phase_t6_mcmc\", nwalkers=50, nsteps=1000\n",
        "    ),\n",
        "    settings=settings,\n",
        "    galaxies=af.CollectionPriorModel(\n",
        "        lens=al.GalaxyModel(\n",
        "            redshift=0.5, bulge=al.lp.EllipticalSersic, mass=al.mp.EllipticalIsothermal\n",
        "        ),\n",
        "        source=al.GalaxyModel(redshift=1.0, bulge=al.lp.EllipticalSersic),\n",
        "    ),\n",
        ")\n",
        "\n",
        "print(\n",
        "    \"Emcee has begun running - checkout the workspace/output\"\n",
        "    \"  folder for live output of the results, images and lens model.\"\n",
        "    \"  This Jupyter notebook cell with progress once Dynesty has completed - this could take some time!\"\n",
        ")\n",
        "\n",
        "# result_mcmc = phase_mcmc.run(dataset=imaging, mask=mask)\n",
        "\n",
        "print(\"Emcee has finished run - you may now continue the notebook.\")\n",
        "\n",
        "# aplt.FitImaging.subplot_fit_imaging(fit=result_mcmc.max_log_likelihood_fit)\n"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}