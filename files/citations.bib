@article{astropy1,
Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
Adsurl = {http://adsabs.harvard.edu/abs/2013A%26A...558A..33A},
Archiveprefix = {arXiv},
Author = {{Astropy Collaboration} and {Robitaille}, T.~P. and {Tollerud}, E.~J. and {Greenfield}, P. and {Droettboom}, M. and {Bray}, E. and {Aldcroft}, T. and {Davis}, M. and {Ginsburg}, A. and {Price-Whelan}, A.~M. and {Kerzendorf}, W.~E. and {Conley}, A. and {Crighton}, N. and {Barbary}, K. and {Muna}, D. and {Ferguson}, H. and {Grollier}, F. and {Parikh}, M.~M. and {Nair}, P.~H. and {Unther}, H.~M. and {Deil}, C. and {Woillez}, J. and {Conseil}, S. and {Kramer}, R. and {Turner}, J.~E.~H. and {Singer}, L. and {Fox}, R. and {Weaver}, B.~A. and {Zabalza}, V. and {Edwards}, Z.~I. and {Azalee Bostroem}, K. and {Burke}, D.~J. and {Casey}, A.~R. and {Crawford}, S.~M. and {Dencheva}, N. and {Ely}, J. and {Jenness}, T. and {Labrie}, K. and {Lim}, P.~L. and {Pierfederici}, F. and {Pontzen}, A. and {Ptak}, A. and {Refsdal}, B. and {Servillat}, M. and {Streicher}, O.},
Doi = {10.1051/0004-6361/201322068},
Eid = {A33},
Eprint = {1307.6212},
Journal = {A\&A},
Keywords = {methods: data analysis, methods: miscellaneous, virtual observatory tools},
Month = oct,
Pages = {A33},
Primaryclass = {astro-ph.IM},
Title = {{Astropy: A community Python package for astronomy}},
Volume = 558,
Year = 2013,
Bdsk-Url-1 = {https://dx.doi.org/10.1051/0004-6361/201322068}}

@article{astropy2,
Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
Adsurl = {https://ui.adsabs.harvard.edu/#abs/2018AJ....156..123T},
Author = {{Price-Whelan}, A.~M. and {Sip{\H{o}}cz}, B.~M. and {G{\"u}nther}, H.~M. and {Lim}, P.~L. and {Crawford}, S.~M. and {Conseil}, S. and {Shupe}, D.~L. and {Craig}, M.~W. and {Dencheva}, N. and {Ginsburg}, A. and {VanderPlas}, J.~T. and {Bradley}, L.~D. and {P{\'e}rez-Su{\'a}rez}, D. and {de Val-Borro}, M. and {Paper Contributors}, (Primary and {Aldcroft}, T.~L. and {Cruz}, K.~L. and {Robitaille}, T.~P. and {Tollerud}, E.~J. and {Coordination Committee}, (Astropy and {Ardelean}, C. and {Babej}, T. and {Bach}, Y.~P. and {Bachetti}, M. and {Bakanov}, A.~V. and {Bamford}, S.~P. and {Barentsen}, G. and {Barmby}, P. and {Baumbach}, A. and {Berry}, K.~L. and {Biscani}, F. and {Boquien}, M. and {Bostroem}, K.~A. and {Bouma}, L.~G. and {Brammer}, G.~B. and {Bray}, E.~M. and {Breytenbach}, H. and {Buddelmeijer}, H. and {Burke}, D.~J. and {Calderone}, G. and {Cano Rodr{\'\i}guez}, J.~L. and {Cara}, M. and {Cardoso}, J.~V.~M. and {Cheedella}, S. and {Copin}, Y. and {Corrales}, L. and {Crichton}, D. and {D{\textquoteright}Avella}, D. and {Deil}, C. and {Depagne}, {\'E}. and {Dietrich}, J.~P. and {Donath}, A. and {Droettboom}, M. and {Earl}, N. and {Erben}, T. and {Fabbro}, S. and {Ferreira}, L.~A. and {Finethy}, T. and {Fox}, R.~T. and {Garrison}, L.~H. and {Gibbons}, S.~L.~J. and {Goldstein}, D.~A. and {Gommers}, R. and {Greco}, J.~P. and {Greenfield}, P. and {Groener}, A.~M. and {Grollier}, F. and {Hagen}, A. and {Hirst}, P. and {Homeier}, D. and {Horton}, A.~J. and {Hosseinzadeh}, G. and {Hu}, L. and {Hunkeler}, J.~S. and {Ivezi{\'c}}, {\v{Z}}. and {Jain}, A. and {Jenness}, T. and {Kanarek}, G. and {Kendrew}, S. and {Kern}, N.~S. and {Kerzendorf}, W.~E. and {Khvalko}, A. and {King}, J. and {Kirkby}, D. and {Kulkarni}, A.~M. and {Kumar}, A. and {Lee}, A. and {Lenz}, D. and {Littlefair}, S.~P. and {Ma}, Z. and {Macleod}, D.~M. and {Mastropietro}, M. and {McCully}, C. and {Montagnac}, S. and {Morris}, B.~M. and {Mueller}, M. and {Mumford}, S.~J. and {Muna}, D. and {Murphy}, N.~A. and {Nelson}, S. and {Nguyen}, G.~H. and {Ninan}, J.~P. and {N{\"o}the}, M. and {Ogaz}, S. and {Oh}, S. and {Parejko}, J.~K. and {Parley}, N. and {Pascual}, S. and {Patil}, R. and {Patil}, A.~A. and {Plunkett}, A.~L. and {Prochaska}, J.~X. and {Rastogi}, T. and {Reddy Janga}, V. and {Sabater}, J. and {Sakurikar}, P. and {Seifert}, M. and {Sherbert}, L.~E. and {Sherwood-Taylor}, H. and {Shih}, A.~Y. and {Sick}, J. and {Silbiger}, M.~T. and {Singanamalla}, S. and {Singer}, L.~P. and {Sladen}, P.~H. and {Sooley}, K.~A. and {Sornarajah}, S. and {Streicher}, O. and {Teuben}, P. and {Thomas}, S.~W. and {Tremblay}, G.~R. and {Turner}, J.~E.~H. and {Terr{\'o}n}, V. and {van Kerkwijk}, M.~H. and {de la Vega}, A. and {Watkins}, L.~L. and {Weaver}, B.~A. and {Whitmore}, J.~B. and {Woillez}, J. and {Zabalza}, V. and {Contributors}, (Astropy},
Doi = {10.3847/1538-3881/aabc4f},
Eid = {123},
Journal = {AJ},
Keywords = {methods: data analysis, methods: miscellaneous, methods: statistical, reference systems, Astrophysics - Instrumentation and Methods for Astrophysics},
Month = Sep,
Pages = {123},
Primaryclass = {astro-ph.IM},
Title = {{The Astropy Project: Building an Open-science Project and Status of the v2.0 Core Package}},
Volume = {156},
Year = 2018,
Bdsk-Url-1 = {https://doi.org/10.3847/1538-3881/aabc4f}
}

@article{PyLops,
abstract = {Linear operators and optimisation are at the core of many algorithms used in signal and image processing, remote sensing, and inverse problems. For small to medium-scale problems, existing software packages (e.g., MATLAB, Python numpy and scipy) allow for explicitly building dense (or sparse) matrices and performing algebraic operations (e.g., computation of matrix-vector products and manipulation of matrices) with syntax that closely represents their corresponding analytical forms. However, many real application, large-scale operators do not lend themselves to explicit matrix representations, usually forcing practitioners to forego of the convenient linear-algebra syntax available for their explicit-matrix counterparts. PyLops is an open-source Python library providing a flexible and scalable framework for the creation and combination of so-called linear operators, class-based entities that represent matrices and inherit their associated syntax convenience, but do not rely on the creation of explicit matrices. We show that PyLops operators can dramatically reduce the memory load and CPU computations compared to explicit-matrix calculations, while still allowing users to seamlessly use their existing knowledge of compact matrix-based syntax that scales to any problem size because no explicit matrices are required.},
archivePrefix = {arXiv},
arxivId = {1907.12349},
author = {Ravasi, Matteo and Vasconcelos, Ivan},
eprint = {1907.12349},
file = {:home/jammy/Documents/Papers/Software/PyLops.pdf:pdf},
title = {{PyLops -- A Linear-Operator Python Library for large scale optimization}},
url = {http://arxiv.org/abs/1907.12349},
year = {2019}
}

@article{colossus,
abstract = {This paper introduces Colossus, a public, open-source python package for calculations related to cosmology, the large-scale structure (LSS) of matter in the universe, and the properties of dark matter halos. The code is designed to be fast and easy to use, with a coherent, well-documented user interface. The cosmology module implements Friedman-Lemaitre-Robertson-Walker cosmologies including curvature, relativistic species, and different dark energy equations of state, and provides fast computations of the linear matter power spectrum, variance, and correlation function. The LSS module is concerned with the properties of peaks in Gaussian random fields and halos in a statistical sense, including their peak height, peak curvature, halo bias, and mass function. The halo module deals with spherical overdensity radii and masses, density profiles, concentration, and the splashback radius. To facilitate the rapid exploration of these quantities, Colossus implements more than 40 different fitting functions from the literature. I discuss the core routines in detail, with particular emphasis on their accuracy. Colossus is available at bitbucket.org/bdiemer/colossus.},
archivePrefix = {arXiv},
arxivId = {1712.04512},
author = {Diemer, Benedikt},
doi = {10.3847/1538-4365/aaee8c},
eprint = {1712.04512},
file = {:home/jammy/Documents/Papers/Software/Collosus2018.pdf:pdf},
issn = {0067-0049},
journal = {The Astrophysical Journal Supplement Series},
keywords = {cosmology,cosmology: theory,methods: numerical,methods,numerical,theory},
number = {2},
pages = {35},
publisher = {IOP Publishing},
title = {{COLOSSUS: A Python Toolkit for Cosmology, Large-scale Structure, and Dark Matter Halos}},
url = {http://dx.doi.org/10.3847/1538-4365/aaee8c},
volume = {239},
year = {2018}
}

@article{corner,
  doi = {10.21105/joss.00024},
  url = {https://doi.org/10.21105/joss.00024},
  year  = {2016},
  month = {jun},
  publisher = {The Open Journal},
  volume = {1},
  number = {2},
  pages = {24},
  author = {Daniel Foreman-Mackey},
  title = {corner.py: Scatterplot matrices in Python},
  journal = {The J. Open Source Softw.}
}
@article{dynesty,
abstract = {We present dynesty, a public, open-source, python package to estimate Bayesian posteriors and evidences (marginal likelihoods) using the dynamic nested sampling methods developed by Higson et al. By adaptively allocating samples based on posterior structure, dynamic nested sampling has the benefits of Markov chain Monte Carlo (MCMC) algorithms that focus exclusively on posterior estimation while retaining nested sampling's ability to estimate evidences and sample from complex, multimodal distributions. We provide an overview of nested sampling, its extension to dynamic nested sampling, the algorithmic challenges involved, and the various approaches taken to solve them in this and previous work. We then examine dynesty's performance on a variety of toy problems along with several astronomical applications. We find in particular problems dynesty can provide substantial improvements in sampling efficiency compared to popular MCMC approaches in the astronomical literature. More detailed statistical results related to nested sampling are also included in the appendix.},
archivePrefix = {arXiv},
arxivId = {1904.02180},
author = {Speagle, Joshua S},
doi = {10.1093/mnras/staa278},
eprint = {1904.02180},
file = {:home/jammy/Documents/Papers/PPLs/Dynesty.pdf:pdf},
issn = {0035-8711},
journal = {MNRAS},
keywords = {data analysis,methods,statistical},
number = {3},
pages = {3132--3158},
title = {{dynesty: a dynamic nested sampling package for estimating Bayesian posteriors and evidences}},
volume = {493},
year = {2020}
}
@article{emcee,
abstract = {We introduce a stable, well tested Python implementation of the affine-invariant ensemble sampler for Markov chain Monte Carlo (MCMC) proposed by Goodman {\&} Weare (2010). The code is open source and has already been used in several published projects in the astrophysics literature. The algorithm behind emcee has several advantages over traditional MCMC sampling methods and it has excellent performance as measured by the autocorrelation time (or function calls per independent sample). One major advantage of the algorithm is that it requires hand-tuning of only 1 or 2 parameters compared to {\$}\backslashsim N{\^{}}2{\$} for a traditional algorithm in an N-dimensional parameter space. In this document, we describe the algorithm and the details of our implementation and API. Exploiting the parallelism of the ensemble method, emcee permits any user to take advantage of multiple CPU cores without extra effort. The code is available online at http://dan.iel.fm/emcee under the MIT License.},
archivePrefix = {arXiv},
arxivId = {1202.3665},
author = {Foreman-Mackey, Daniel and Hogg, David W. and Lang, Dustin and Goodman, Jonathan},
doi = {10.1086/670067},
eprint = {1202.3665},
file = {:home/jammy/Documents/Papers/PPLs/Emcee.pdf:pdf},
issn = {00046280},
journal = {Publ. Astron. Soc. Pac.},
number = {925},
pages = {306--312},
title = {{emcee : The MCMC Hammer }},
volume = {125},
year = {2013}
}
@article{matplotlib,
  Author    = {Hunter, J. D.},
  Title     = {Matplotlib: A 2D graphics environment},
  Journal   = {Comput Sci Eng},
  Volume    = {9},
  Number    = {3},
  Pages     = {90--95},
  abstract  = {Matplotlib is a 2D graphics package used for Python for
  application development, interactive scripting, and publication-quality
  image generation across user interfaces and operating systems.},
  publisher = {IEEE COMPUTER SOC},
  doi       = {10.1109/MCSE.2007.55},
  year      = 2007
}
@article{numba,
abstract = {Dynamic, interpreted languages, like Python, are attractive for domain-experts and scientists experimenting with new ideas. However, the performance of the interpreter is of-ten a barrier when scaling to larger data sets. This paper presents a just-in-time compiler for Python that focuses in scientific and array-oriented computing. Starting with the simple syntax of Python, Numba compiles a subset of the language into efficient machine code that is comparable in performance to a traditional compiled language. In addi-tion, we share our experience in building a JIT compiler using LLVM[1].},
author = {Lam, Siu Kwan and Pitrou, Antoine and Seibert, Stanley},
doi = {10.1145/2833157.2833162},
file = {:home/jammy/Documents/Papers/Software/numba{\_}sc15.pdf:pdf},
isbn = {9781450340052},
journal = {Proceedings of the Second Workshop on the LLVM Compiler Infrastructure in HPC - LLVM '15},
keywords = {2,a jit for numeric,com-,compiler,jit,just-in-time,llvm,numba is a function-at-a-time,python},
pages = {1--6},
title = {{Numba: a LLVM-based Python JIT compiler}},
url = {http://dl.acm.org/citation.cfm?doid=2833157.2833162},
year = {2015}
}
@article{numpy,
  author={S. {van der Walt} and S. C. {Colbert} and G. {Varoquaux}},
  doi={10.1109/MCSE.2011.37},
  journal={Comput Sci Eng},
  title={The NumPy Array2D: A Structure for Efficient Numerical Computation},
  year={2011},
  volume={13},
  number={2},
  pages={22-30},}
@article{pyautofit,
  doi = {10.21105/joss.02550},
  url = {https://doi.org/10.21105/joss.02550},
  year = {2021},
  publisher = {The Open Journal},
  volume = {6},
  number = {58},
  pages = {2550},
  author = {Nightingale, J. W. and Hayes, R. G. and Griffiths, M.},
  title = {`PyAutoFit`: A Classy Probabilistic Programming Language for Model Composition and Fitting},
  journal = {J. Open Source Softw.}
}
@article{pyautogalaxy,
  doi = {10.21105/joss.04475},
  url = {https://doi.org/10.21105/joss.04475},
  year = {2023},
  publisher = {The Open Journal},
  volume = {8},
  number = {81},
  pages = {4475},
  author = {James. W. Nightingale and Aristeidis Amvrosiadis and Richard G. Hayes and Qiuhan He and Amy Etherington and XiaoYue Cao and Shaun Cole and Jonathan Frawley and Carlos S. Frenk and Sam Lange and Ran Li and Richard J. Massey and Mattia Negrello and Andrew Robertson},
  title = {PyAutoGalaxy: Open-Source Multiwavelength Galaxy Structure & Morphology},
  journal = {J. Open Source Softw.}
 }
@article{pyautolens,
  doi = {10.21105/joss.02825},
  url = {https://doi.org/10.21105/joss.02825},
  year = {2021},
  publisher = {The Open Journal},
  volume = {6},
  number = {58},
  pages = {2825},
  author = {Nightingale, J. W. and Hayes, R. G. and Ashley Kelly and Aristeidis Amvrosiadis and Amy Etherington and Qiuhan He and Nan Li and XiaoYue Cao and Jonathan Frawley and Shaun Cole and Andrea Enia and Carlos S. Frenk and David R. Harvey and Ran Li and Richard J. Massey and Mattia Negrello and Andrew Robertson},
  title = {`PyAutoLens`: Open-Source Strong Gravitational Lensing},
  journal = {J. Open Source Softw.}
}
@article{pynufft,
abstract = {A Python non-uniform fast Fourier transform (PyNUFFT) package has been developed to accelerate multidimensional non-Cartesian image reconstruction on heterogeneous platforms. Since scientific computing with Python encompasses a mature and integrated environment, the time efficiency of the NUFFT algorithm has been a major obstacle to real-time non-Cartesian image reconstruction with Python. The current PyNUFFT software enables multi-dimensional NUFFT accelerated on a heterogeneous platform, which yields an efficient solution to many non-Cartesian imaging problems. The PyNUFFT also provides several solvers, including the conjugate gradient method, 1 total variation regularized ordinary least square (L1TV-OLS), and 1 total variation regularized least absolute deviation (L1TV-LAD). Metaprogramming libraries have been employed to accelerate PyNUFFT. The PyNUFFT package has been tested on multi-core central processing units (CPUs) and graphic processing units (GPUs), with acceleration factors of 6.3–9.5× on a 32-thread CPU platform and 5.4–13× on a GPU.},
author = {Lin, Jyh Miin},
doi = {10.3390/jimaging4030051},
file = {:home/jammy/Documents/Papers/Software/jimaging-04-00051-v2.pdf:pdf},
issn = {2313433X},
journal = {Journal of Imaging},
keywords = {Graphic processing unit (GPU),Heterogeneous system architecture (HSA),Magnetic resonance imaging (MRI),Multi-core system,Total variation (TV)},
number = {3},
pages = {1--22},
title = {{Python non-uniform fast fourier transform (PyNUFFT): An accelerated non-cartesian MRI package on a heterogeneous platform (CPU/GPU)}},
volume = {4},
year = {2018}
}
@article{pyswarms,
    author  = {Lester James V. Miranda},
    title   = "{P}y{S}warms, a research-toolkit for {P}article {S}warm {O}ptimization in {P}ython",
    journal = {J. Open Source Softw.},
    year    = {2018},
    volume  = {3},
    issue   = {21},
    doi     = {10.21105/joss.00433},
    url     = {https://doi.org/10.21105/joss.00433}
}
 @book{python,
 author = {Van Rossum, Guido and Drake, Fred L.},
 title = {Python 3 Reference Manual},
 year = {2009},
 isbn = {1441412697},
 publisher = {CreateSpace},
 address = {Scotts Valley, CA}
}
@article{scikit-image,
  title={scikit-image: image processing in Python},
  author={Van der Walt, Stefan and Sch{\"o}nberger, Johannes L and Nunez-Iglesias, Juan and Boulogne, Fran{\c{c}}ois and Warner, Joshua D and Yager, Neil and Gouillart, Emmanuelle and Yu, Tony},
  journal={PeerJ},
  volume={2},
  pages={e453},
  year={2014},
  publisher={PeerJ Inc.}
}
@article{scikit-learn,
 title={Scikit-learn: Machine Learning in {P}ython},
 author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.
         and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.
         and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and
         Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
 journal={Journal of Machine Learning Research},
 volume={12},
 pages={2825--2830},
 year={2011}
}
@article{scipy,
       author = {{Virtanen}, Pauli and {Gommers}, Ralf and {Oliphant},
         Travis E. and {Haberland}, Matt and {Reddy}, Tyler and
         {Cournapeau}, David and {Burovski}, Evgeni and {Peterson}, Pearu
         and {Weckesser}, Warren and {Bright}, Jonathan and {van der Walt},
         St{\'e}fan J.  and {Brett}, Matthew and {Wilson}, Joshua and
         {Jarrod Millman}, K.  and {Mayorov}, Nikolay and {Nelson}, Andrew
         R.~J. and {Jones}, Eric and {Kern}, Robert and {Larson}, Eric and
         {Carey}, CJ and {Polat}, {\.I}lhan and {Feng}, Yu and {Moore},
         Eric W. and {Vand erPlas}, Jake and {Laxalde}, Denis and
         {Perktold}, Josef and {Cimrman}, Robert and {Henriksen}, Ian and
         {Quintero}, E.~A. and {Harris}, Charles R and {Archibald}, Anne M.
         and {Ribeiro}, Ant{\^o}nio H. and {Pedregosa}, Fabian and
         {van Mulbregt}, Paul and {Contributors}, SciPy 1. 0},
        title = "{SciPy 1.0: Fundamental Algorithms for Scientific
                  Computing in Python}",
      journal = {Nature Methods},
      year = "2020",
      volume={17},
      pages={261--272},
      adsurl = {https://rdcu.be/b08Wh},
      doi = {10.1038/s41592-019-0686-2},
}
@misc{sqlite,
  title={{SQLite}},
  url={https://www.sqlite.org/index.html},
  version={3.31.1},
  year={2020},
  author={Hipp, Richard D}
}
@ARTICLE{ultranest,
       author = {{Buchner}, Johannes},
        title = "{UltraNest - a robust, general purpose Bayesian inference engine}",
      journal = {The J. Open Source Softw.},
     keywords = {C, Monte Carlo, Python, Nested Sampling, C++, Bayesian inference, Fortran, Bayes factors, Statistics - Computation, Astrophysics - Instrumentation and Methods for Astrophysics},
         year = 2021,
        month = apr,
       volume = {6},
       number = {60},
          eid = {3001},
        pages = {3001},
          doi = {10.21105/joss.03001},
archivePrefix = {arXiv},
       eprint = {2101.09604},
 primaryClass = {stat.CO},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2021JOSS....6.3001B},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}
@article{zeus1,
    title={zeus: A Python Implementation of the Ensemble Slice Sampling method},
    author={Minas Karamanis and Florian Beutler},
    year={2021},
    note={in prep}
}

@article{zeus2,
    title={Ensemble Slice Sampling},
    author={Minas Karamanis and Florian Beutler},
    year={2020},
    eprint={2002.06212},
    archivePrefix={arXiv},
    primaryClass={stat.ML}
}
@article{Nightingale2015,
abstract = {We present a new pixelized method for the inversion of gravitationally lensed extended source images which we term adaptive semi-linear inversion (SLI). At the heart of the method is an h-means clustering algorithm which is used to derive a source plane pixelization that adapts to the lens model magnification. The distinguishing feature of adaptive SLI is that every pixelization is derived from a random initialization, ensuring that data discretization is performed in a completely different and unique way for every model parameter set. We compare standard SLI on a fixed source pixel grid with the new method and demonstrate the shortcomings of the former when modelling singular power-law ellipsoid (SPLE) lens profiles. In particular, we demonstrate the superior reliability and efficiency of adaptive SLI which, by design, fixes the number of degrees of freedom (NDOF) of the optimization and thereby removes biases present with other methods that allow the NDOF to vary. In addition, we highlight the importance of data discretization in pixel-based inversion methods, showing that adaptive SLI averages over significant systematics that are present when a fixed source pixel grid is used. In the case of the SPLE lens profile, we show how the method successfully samples its highly degenerate posterior probability distribution function with a single nonlinear search. The robustness of adaptive SLI provides a firm foundation for the development of a strong lens modelling pipeline, which will become necessary in the short-term future to cope with the increasing rate of discovery of new strong lens systems.},
archivePrefix = {arXiv},
arxivId = {1412.7436},
author = {Nightingale, J. W. and Dye, S.},
doi = {10.1093/mnras/stv1455},
eprint = {1412.7436},
issn = {13652966},
journal = {MNRAS},
keywords = {galaxies: evolution,galaxies: structure,Methods: observational},
month = {sep},
number = {3},
pages = {2940--2959},
title = {{Adaptive semi-linear inversion of strong gravitational lens imaging}},
volume = {452},
year = {2015}
}
@article{Nightingale2018,
abstract = {This work presents AutoLens, the first entirely automated modeling suite for the analysis of galaxy-scale strong gravitational lenses. AutoLens simultaneously models the lens galaxy's light and mass whilst reconstructing the extended source galaxy on an adaptive pixel-grid. The method's approach to source-plane discretization is amorphous, adapting its clustering and regularization to the intrinsic properties of the lensed source. The lens's light is fitted using a superposition of Sersic functions, allowing AutoLens to cleanly deblend its light from the source. Single-component mass models representing the lens's total mass density profile are demonstrated, which in conjunction with light modeling can detect central images using a centrally cored profile. Decomposed mass modeling is also shown, which can fully decouple a lens's light and dark matter and determine whether the two components are geometrically aligned. The complexity of the light and mass models is automatically chosen via Bayesian model comparison. These steps form AutoLens's automated analysis pipeline, such that all results in this work are generated without any user intervention. This is rigorously tested on a large suite of simulated images, assessing its performance on a broad range of lens profiles, source morphologies, and lensing geometries. The method's performance is excellent, with accurate light, mass, and source profiles inferred for data sets representative of both existing Hubble imaging and future Euclid wide-field observations.},
archivePrefix = {arXiv},
arxivId = {1708.07377},
author = {Nightingale, J. W. and Dye, S. and Massey, Richard J.},
doi = {10.1093/mnras/sty1264},
eprint = {1708.07377},
file = {:home/jammy/Documents/Papers{\_}Me/AutoLensChangesMarked.pdf:pdf},
issn = {13652966},
journal = {MNRAS},
keywords = {Galaxy: structure,Gravitational lensing,Methods: data analysis},
number = {4},
pages = {4738--4784},
title = {{AutoLens: Automated modeling of a strong lens's light, mass, and source}},
url = {https://academic.oup.com/mnras/article/478/4/4738/5001434},
volume = {478},
year = {2018}
}
@article{Nightingale2019,
abstract = {We investigate how strong gravitational lensing can test contemporary models of massive elliptical (ME) galaxy formation, by combining a traditional decomposition of their visible stellar distribution with a lensing analysis of their mass distribution. As a proof of concept, we study a sample of three ME lenses, observing that all are composed of two distinct baryonic structures, a 'red' central bulge surrounded by an extended envelope of stellar material. Whilst these two components look photometrically similar, their distinct lensing effects permit a clean decomposition of their mass structure. This allows us to infer two key pieces of information about each lens galaxy: (i) the stellar mass distribution (without invoking stellar populations models) and (ii) the inner dark matter halo mass. We argue that these two measurements are crucial to testing models of ME formation, as the stellar mass profile provides a diagnostic of baryonic accretion and feedback whilst the dark matter mass places each galaxy in the context of LCDM large-scale structure formation. We also detect large rotational offsets between the two stellar components and a lopsidedness in their outer mass distributions, which hold further information on the evolution of each ME. Finally, we discuss how this approach can be extended to galaxies of all Hubble types and what implication our results have for studies of strong gravitational lensing.},
archivePrefix = {arXiv},
arxivId = {1901.07801},
author = {Nightingale, J. W. and Massey, Richard J. and Harvey, David R. and Cooper, Andrew P. and Etherington, Amy and Tam, Sut Ieng and Hayes, Richard G.},
doi = {10.1093/mnras/stz2220},
eprint = {1901.07801},
file = {:home/jammy/Documents/Papers{\_}Me/Gal{\_}Structure{\_}Final/GalaxyStructure.pdf:pdf},
issn = {13652966},
journal = {MNRAS},
keywords = {galaxies: Evolution,galaxies: Formation,Gravitational lensing: Strong},
number = {2},
pages = {2049--2068},
title = {{Galaxy structure with strong gravitational lensing: Decomposing the internal mass distribution of massive elliptical galaxies}},
url = {http://arxiv.org/abs/1901.07801},
volume = {489},
year = {2019}
}
@article{Anowar2019,
    author = {Shajib, Anowar J},
    title = "{Unified lensing and kinematic analysis for any elliptical mass profile}",
    journal = {MNRAS},
    volume = {488},
    number = {1},
    pages = {1387-1400},
    year = {2019},
    month = {07},
    abstract = "{We demonstrate an efficient method to compute the strong-gravitational-lensing deflection angle and magnification for any elliptical surface density profile. This method solves a numerical hurdle in lens modelling that has lacked a general solution for nearly three decades. The hurdle emerges because it is prohibitive to derive analytic expressions of the lensing quantities for most elliptical mass profiles. In our method, we first decompose an elliptical mass profile into concentric Gaussian components. We introduce an integral transform that provides us with a fast and accurate algorithm for this Gaussian decomposition. We derive analytic expressions of the lensing quantities for a Gaussian component. As a result, we can compute these quantities for the total mass profile by adding up the contributions from the individual components. This lensing analysis self-consistently completes the kinematic description in terms of Gaussian components presented by Cappellari (2008). Our method is general without extra computational burden unlike other methods currently in use.}",
    issn = {0035-8711},
    doi = {10.1093/mnras/stz1796},
    url = {https://doi.org/10.1093/mnras/stz1796},
    eprint = {https://academic.oup.com/mnras/article-pdf/488/1/1387/28937972/stz1796.pdf},
}

@INPROCEEDINGS{dynesty1,
       author = {{Skilling}, John},
        title = "{Nested Sampling}",
     keywords = {02.50.Tt, Inference methods},
    booktitle = {Bayesian Inference and Maximum Entropy Methods in Science and Engineering: 24th International Workshop on Bayesian Inference and Maximum Entropy Methods in Science and Engineering},
         year = 2004,
       editor = {{Fischer}, Rainer and {Preuss}, Roland and {Toussaint}, Udo Von},
       series = {American Institute of Physics Conference Series},
       volume = {735},
        month = nov,
        pages = {395-405},
          doi = {10.1063/1.1835238},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2004AIPC..735..395S},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}


@article{dynesty2,
abstract = {Nested sampling estimates directly how the likelihood function relates to prior mass. The evidence (alternatively the marginal likelihood, marginal density of the data, or the prior predictive) is immediately obtained by summation. It is the prime result of the computation, and is accompanied by an estimate of numerical uncertainty. Samples from the posterior distribution are an optional by-product, obtainable for any temperature. The method relies on sampling within a hard constraint on likelihood value, as opposed to the softened likelihood of annealing methods. Progress depends only on the shape of the "nested"contours of likelihood, and not on the likelihood values. This invariance (over monotonic relabelling) allows the method to deal with a class of phasechange problems which effectively defeat thermal annealing. {\textcopyright} 2006 International Society for Bayesian Analysis.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Skilling, John},
doi = {10.1214/06-BA127},
eprint = {arXiv:1011.1669v3},
isbn = {9788578110796},
issn = {19360975},
journal = {Bayesian Analysis},
keywords = {Algorithm,Annealing,Bayesian computation,Evidence,Marginal likelihood,Model selection,Nest,Phase change},
number = {4},
pages = {833--860},
pmid = {25246403},
title = {{Nested sampling for general Bayesian computation}},
url = {http://projecteuclid.org/euclid.ba/1340370944},
volume = {1},
year = {2006}
}

@article{dynesty3,
abstract = {We present further development and the first public release of our multimodal nested sampling algorithm, called MultiNest. This Bayesian inference tool calculates the evidence, with an associated error estimate, and produces posterior samples from distributions that may contain multiple modes and pronounced (curving) degeneracies in high dimensions. The developments presented here lead to further substantial improvements in sampling efficiency and robustness, as compared to the original algorithm presented in Feroz and Hobson, which itself significantly outperformed existing Markov chain Monte Carlo techniques in a wide range of astrophysical inference problems. The accuracy and economy of the MultiNest algorithm are demonstrated by application to two toy problems and to a cosmological inference problem focusing on the extension of the vanilla $\Lambda$ cold dark matter model to include spatial curvature and a varying equation of state for dark energy. The MultiNest software, which is fully parallelized using MPI and includes an interface to CosmoMC, is available at http://www.mrao.cam.ac.uk/software/multinest/. It will also be released as part of the SuperBayeS package, for the analysis of supersymmetric theories of particle physics, at http://www.superbayes.org. {\textcopyright} 2009 RAS.},
archivePrefix = {arXiv},
arxivId = {0809.3437},
author = {Feroz, F. and Hobson, M. P. and Bridges, M.},
doi = {10.1111/j.1365-2966.2009.14548.x},
eprint = {0809.3437},
isbn = {0035-8711},
issn = {00358711},
journal = {MNRAS},
keywords = {Methods: Data analysis,Methods: Statistical},
number = {4},
pages = {1601--1614},
pmid = {29176},
title = {{MultiNest: An efficient and robust Bayesian inference tool for cosmology and particle physics}},
volume = {398},
year = {2009}
}

@article{dynesty4,
title={joshspeagle/dynesty: v2.1.2}, DOI={10.5281/zenodo.7995596},
abstractNote={<p>This is a bug release mostly concerning the checkpointing. Check the changelog for more details.</p>},
publisher={Zenodo},
author={Sergey Koposov and Josh Speagle and Kyle Barbary and Gregory Ashton and Ed Bennett and Johannes Buchner
and Carl Scheffler and Ben Cook and Colm Talbot and James Guillochon and et al.}, year={2023}, month={Jun} }